# AUTOGENERATED! DO NOT EDIT! File to edit: Projet 2_CNN model_HARISS_Shiny.ipynb.

# %% auto 0
__all__ = []

# %% Projet 2_CNN model_HARISS_Shiny.ipynb 1
import numpy as np
from scipy.stats import norm, skewnorm, bootstrap, t, beta
import matplotlib.pyplot as plt
from random import sample
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data.dataloader import DataLoader
from torchvision import datasets, transforms
import pandas as pd
from scipy.stats.mstats import mquantiles
from tqdm import tqdm
from PIL import Image
import io
import warnings
from numba import jit
import streamlit as st
from streamlit_jupyter import StreamlitPatcher, tqdm
import sys
import streamlit.components.v1 as components
from local_components import st_gtag

st_gtag(id="G-00VKKMT6HT")  # Initialisation de Google Analytics

# %% Projet 2_CNN model_HARISS_Shiny.ipynb 3
tab1, tab2 = st.tabs(["App","Legal notice and disclosure"])

with tab2:
    st.subheader("Legal notice:")
    st.markdown("In accordance with the provisions of French Law No. 2004-575 of June 21, 2004 on confidence in the digital economy, users of this site are informed of the identity of the various parties involved in its implementation and monitoring.")
    st.markdown("Editor:  \n  Kevin Le Boedec  \n  137 avenue du general Leclerc  \n  92340 Bourg La Reine (France)  \n  Email: drleboedec@hotmail.fr")
    st.markdown("Host:  \n  Streamlit.io by Snowflake Inc.  \n  106 E Babcock St Bozeman MT 59715 (USA)  \n  Email: david.amoriell@snowflake.com")
    st.subheader("Personal data and cookies:")
    st.markdown("We do not collect any private information. We use cookies which are intended to signal your visit to our site. These cookies do not identify you personally but, in general, record information relating to the navigation of your computer on the site (pages consulted, dates and times of consultation, etc.). They will in no case be transferred to third parties. A cookie is simply a means of recording site statistics to better understand usage patterns. Your identity cannot be known. We remind you that your browser has features that allow you to oppose the registration of cookies, to be warned before accepting cookies, or to delete them. You can disable or delete cookies by changing the settings of your Internet browser.")
    st.subheader("Disclosure:")
    st.markdown("The creator of HARISS is not to be held responsible for any clinical decisions that may come as a result of the model estimation. The reliability of a reference interval does not only depend on the statistical method used, but also on the quality of the data used to construct it.")

with tab1:
    
    st.title(" :blue[HARISS] ")
    st.markdown(" :blue[Histogram Analyzer for Reference Intervals of Small Samples] ")
    
    with st.popover("How does it work?", use_container_width=True):
        st.markdown("Upload your laboratory data in Excel file format. The Excel file should be formatted as below:")
        st.image("Excel file example.jpg")
        st.markdown("Only the parameters of interest should be in the Excel file (remove ID numbers or other variables not meant to be analyzed).")
        st.markdown("Your raw data will appear in a table, followed by the data distribution histogram, the predicted data distribution, and the 95% reference interval (RI) with the 90% confidence interval of each RI limit for each laboratory parameter.")
        st.markdown("To analyze another set of data, just upload another Excel file and the previous results will be overwritten.")
        st.markdown("[HARISS](https://doi.org/10.1111/vcp.70033) uses a Convolutional Neural Network to predict the shape of a population distribution from a sample distribution histogram. It has been trained to analyze data distribution histograms from samples of sizes ranging from 20 to 40 individuals. The 95% RI provided is built according to the method described in [Coisnon et al](https://onlinelibrary.wiley.com/doi/abs/10.1111/vcp.13000). The 90% confidence interval of each estimated RI limit is calculated based on 200 bootstrap iterations.")

    # %% Projet 2_CNN model_HARISS_Shiny.ipynb 4
    file = st.file_uploader("upload excel file", type={"xlsx"})
    if file is not None:
        df = pd.read_excel(file)
        # Envoi d'un événement Google Analytics lors du chargement du fichier
        st_gtag(type="event", event_name="file_uploaded",)
    else:
        sys.exit(0)
    
    #    df = pd.DataFrame()
    st.write(df)
    
    # %% Projet 2_CNN model_HARISS_Shiny.ipynb 6
    transform = transforms.Compose([transforms.ToTensor(), transforms.Grayscale(num_output_channels=1), transforms.Normalize((0.5,),(0.5,)), transforms.Resize((24,32))]) #au lieu de 480,640
    
    # %% Projet 2_CNN model_HARISS_Shiny.ipynb 7
    class LeNet(nn.Module):
        def __init__(self):
            super(LeNet, self).__init__()
            self.conv1 = nn.Conv2d(1,16,5,padding='valid')  #  (# of channels in the input image = 1 car image N&B, # of filters = 16, kernel_size = 5 [car 5x5], pas de padding c'est a dire pas de cadre autour des donnees)
            self.batchnorm1 = nn.BatchNorm2d(16)
            self.act1 =torch.nn.ReLU()
    #        self.act1 =torch.nn.LeakyReLU(negative_slope=0.1)
            self.maxpool1 = nn.MaxPool2d(2)
            self.conv2 = nn.Conv2d(16,32,5,padding='valid')
            self.batchnorm2 = nn.BatchNorm2d(32)
            self.act2 =torch.nn.ReLU()
    #        self.act2 =torch.nn.LeakyReLU(negative_slope=0.1)
            self.maxpool2 = nn.MaxPool2d(2)
            self.un = nn.Linear(3*5*32,100)
            self.act3 =torch.nn.ReLU()
    #        self.act3 =torch.nn.LeakyReLU(negative_slope=0.1)
            self.deux = nn.Linear(100,3)
    #        self.dropout = nn.Dropout(0.5)
    
        def forward(self, x):
            # YOUR CODE
            h = self.maxpool2(self.batchnorm2(self.act2(self.conv2(self.maxpool1(self.batchnorm1(self.act1(self.conv1(x))))))))
            h = h.view(-1,3*5*32)  # cf la taille de la sortie de self.fc1
    #        h = self.dropout(h)
            return self.deux(self.act3(self.un(h)))
    
        def num_flat_features(self, x):
            size = x.size()[1:]  # all dimensions except the batch dimension
            num_features = 1
            for s in size:
                num_features *= s
            return num_features
    
    # %% Projet 2_CNN model_HARISS_Shiny.ipynb 8
    model2 = torch.load('model.pth', weights_only=False)
    
    # %% Projet 2_CNN model_HARISS_Shiny.ipynb 10
    hist=[]
    for i in range(df.shape[1]):
        fig=plt.hist(df[df.columns[i]].dropna(), edgecolor = "black", color="white")
        plt.axis('off')
        img_buf = io.BytesIO()
        plt.savefig(img_buf, format='jpg')
        hist.append(Image.open(img_buf))
        plt.show()
        plt.close()
    
    
    # %% Projet 2_CNN model_HARISS_Shiny.ipynb 11
    fig, axes = plt.subplots(nrows=len(hist), ncols=1)
    if len(hist) == 1:
        axes = [axes]
    for i in range(len(hist)):
        axes[i].imshow(hist[i])
        axes[i].set_title(f"{df.columns[i]}")
        axes[i].set_xticks([])
        axes[i].set_yticks([])
    fig.set_size_inches(20, 20)
    fig.tight_layout()
    
    #st.pyplot(fig)
    
    # %% Projet 2_CNN model_HARISS_Shiny.ipynb 12
    class MyDataset(torch.utils.data.Dataset):
        def __init__(self, img_list, transform):
            super(MyDataset, self).__init__()
            self.img_list = img_list
            self.transform = transform
            
        def __len__(self):
            return len(self.img_list)
    
        def __getitem__(self, idx):
            img = self.img_list[idx]
            return self.transform(img)
    
    # %% Projet 2_CNN model_HARISS_Shiny.ipynb 13
    dataset=MyDataset(hist, transform)
    
    # %% Projet 2_CNN model_HARISS_Shiny.ipynb 14
    loader = DataLoader(dataset,batch_size=df.shape[1])
    
    # %% Projet 2_CNN model_HARISS_Shiny.ipynb 15
    keys={0:"Gaussian", 1:"Left-skewed", 2:"Lognormal"}
    
    # %% Projet 2_CNN model_HARISS_Shiny.ipynb 16
    refZ = norm.ppf(1 - ((1 - 0.95)/2))
    
    # %% Projet 2_CNN model_HARISS_Shiny.ipynb 17
    def robust(data, indices=None, refConf=0.95):
        if indices is None:
            indices = range(len(data))
        data = np.sort(data[indices])
        n = len(data)
        median = np.nanmedian(data)
        Tbi = median
        TbiNew = 10000
        c = 3.7
        MAD = np.nanmedian(np.abs(data - median)) / 0.6745
        smallDiff = False
        while not smallDiff:
            ui = (data - Tbi) / (c * MAD)
            ui[ui < -1] = 1
            ui[ui > 1] = 1
            wi = np.zeros_like(ui)
            mask = (ui >= -1) & (ui <= 1)
            wi[mask] = (1 - ui[mask]**2)**2
            Tbi_sum_wi = np.sum(wi)
            if Tbi_sum_wi == 0: break
            TbiNew = np.sum(data * wi)/Tbi_sum_wi
            if not np.isfinite(TbiNew) or np.abs(TbiNew - Tbi) < 0.000001:
                break
            Tbi = TbiNew
        ui = (data - median) / (205.6 * MAD)
        sbi205_6 = 205.6 * MAD * np.sqrt((n * np.sum(((1 - ui[(ui > -1) & (ui < 1)]**2)**4) * ui[(ui > -1) & (ui < 1)]**2)) /
                                         (np.sum((1 - ui[(ui > -1) & (ui < 1)]**2) * (1 - 5 * ui[(ui > -1) & (ui < 1)]**2)) *
                                          max(1, -1 + np.sum((1 - ui[(ui > -1) & (ui < 1)]**2) * (1 - 5 * ui[(ui > -1) & (ui < 1)]**2)))))
    
        ui = (data - median) / (3.7 * MAD)
        sbi3_7 = 3.7 * MAD * np.sqrt((n * np.sum(((1 - ui[(ui > -1) & (ui < 1)]**2)**4) * ui[(ui > -1) & (ui < 1)]**2)) /
                                     (np.sum((1 - ui[(ui > -1) & (ui < 1)]**2) * (1 - 5 * ui[(ui > -1) & (ui < 1)]**2)) *
                                      max(1, -1 + np.sum((1 - ui[(ui > -1) & (ui < 1)]**2) * (1 - 5 * ui[(ui > -1) & (ui < 1)]**2)))))
    
        ui = (data - Tbi) / (3.7 * sbi3_7)
        St3_7 = 3.7 * sbi3_7 * np.sqrt((np.sum(((1 - ui[(ui > -1) & (ui < 1)]**2)**4) * ui[(ui > -1) & (ui < 1)]**2)) /
                                       (np.sum((1 - ui[(ui > -1) & (ui < 1)]**2) * (1 - 5 * ui[(ui > -1) & (ui < 1)]**2)) *
                                        max(1, -1 + np.sum((1 - ui[(ui > -1) & (ui < 1)]**2) * (1 - 5 * ui[(ui > -1) & (ui < 1)]**2)))))
    
        t_statistic = t.ppf(1 - (1 - refConf) / 2, n - 1)
        margin = t_statistic * np.sqrt(sbi205_6**2 + St3_7**2)
        robust_lower = Tbi - margin
        robust_upper = Tbi + margin
        ref_interval = (robust_lower, robust_upper)
    
        return ref_interval
    
    # %% Projet 2_CNN model_HARISS_Shiny.ipynb 18
    @jit(nopython=True)
    def bootstrap_quantiles(data, iterations, quantile):
        sample_size = len(data)
        quantiles = np.zeros(iterations)
        for i in range(iterations):
            sample = np.random.choice(data, size=sample_size, replace=True)
            quantiles[i] = np.quantile(sample, quantile)
        return quantiles

    def harrell_davis(data, quantile):
        n = len(data)
        sorted_data = np.sort(data)
        a, b = (n + 1) * quantile, (n + 1) * (1 - quantile)
        i = np.arange(1, n + 1)
        weights = beta.cdf(i / n, a, b) - beta.cdf((i - 1) / n, a, b)
        return np.sum(weights * sorted_data)

    def bca_correction(theta_hat, theta_boot, data, func):
        # 1. Calcul du biais z0
        # On évite 0 ou 1 pour le ratio pour que norm.ppf ne renvoie pas l'infini
        n_boot = len(theta_boot)
        prop = np.sum(theta_boot < theta_hat) / n_boot
        prop = np.clip(prop, 1/n_boot, 1 - 1/n_boot)
        z0 = norm.ppf(prop)
        # 2. Accélération a (Jackknife)
        n = len(data)
        # On s'assure que data est bien un array numpy pour np.delete
        data_arr = np.asarray(data)
        jack = np.array([func(np.delete(data_arr, j)) for j in range(n)])
        m_j = np.mean(jack)
        diff = m_j - jack
        sum_sq = np.sum(diff**2)
        sum_cub = np.sum(diff**3)
        # Calcul de a avec sécurité sur la division
        if sum_sq > 0:
            a = sum_cub / (6 * (sum_sq**1.5))
        else:
            a = 0
        # 3. Fonction de transformation des quantiles
        def get_corrected_q(alpha):
            z_alpha = norm.ppf(alpha)
            num = z0 + z_alpha
            den = 1 - a * num
            # Sécurité critique : si le dénominateur est nul ou proche de zéro
            if np.abs(den) < 1e-10:
                return alpha # On replie sur le quantile standard si le calcul échoue    
            new_z = z0 + (num / den)
            new_q = norm.cdf(new_z)
            # Sécurité finale : forcer strictement entre 0 et 1 (excluant les bords exacts)
            return np.clip(new_q, 0.0001, 0.9999)
        # Calcul des indices corrigés
        q_low = get_corrected_q(0.05)
        q_up = get_corrected_q(0.95)
        # Vérification ultime avant l'appel à numpy
        if not (0 <= q_low <= 1) or not (0 <= q_up <= 1) or np.isnan(q_low) or np.isnan(q_up):
            return np.quantile(theta_boot, 0.05), np.quantile(theta_boot, 0.95)
        return np.quantile(theta_boot, q_low), np.quantile(theta_boot, q_up)
        
    def get_outlier(x,tukeymultiplier=2):
        Q1=x.quantile(.25)
        Q3=x.quantile(.75)
        IQR=Q3-Q1
        lowerlimit = Q1 - tukeymultiplier*IQR
        upperlimit = Q3 + tukeymultiplier*IQR
        is_outlier = (x < lowerlimit) | (x > upperlimit)
        return is_outlier.any()
        
    # %% Projet 2_CNN model_HARISS_Shiny.ipynb 19
    warnings.filterwarnings('ignore') 
    for images in loader:
        pred = model2(images)
        result=pred.argmax(-1)
        lower=np.zeros(result.size(dim=0))
        upper=np.zeros(result.size(dim=0))
        lower90_low=np.zeros(result.size(dim=0))
        upper90_low=np.zeros(result.size(dim=0))
        lower90_up=np.zeros(result.size(dim=0))
        upper90_up=np.zeros(result.size(dim=0))
        method_lower=[]
        method_upper=[]
        for i in range(result.size(dim=0)):
            current_data = df.iloc[:,i].dropna().values
            if result[i].item()==0:
                lower[i]=np.nanmean(current_data) - refZ * np.nanstd(current_data)
                upper[i]=np.nanmean(current_data) + refZ * np.nanstd(current_data)
                btlower=np.zeros(200)
                btupper=np.zeros(200)
                f_l_boot = lambda d: np.mean(d) - refZ * np.std(d)
                f_u_boot = lambda d: np.mean(d) + refZ * np.std(d)
                method_lower = "Parametric method"
                method_upper = "Parametric method"
                for f in range(200):
                    sample_data = np.random.choice(current_data, replace=True, size=len(current_data))
                    btlower[f]=f_l_boot(sample_data)
                    btupper[f]=f_u_boot(sample_data)
                lower90_low[i], lower90_up[i]=bca_correction(lower[i], btlower, current_data, f_l_boot)
                upper90_low[i], upper90_up[i]=bca_correction(upper[i], btupper, current_data, f_u_boot)
            elif result[i].item()==2:
                lower[i]=mquantiles(current_data,prob=(0.025),alphap=0, betap=0)[0]
 #               lower[i] = harrell_davis(df.values[:, i], 0.025)
                btnp=np.zeros(10000)
                btnp = bootstrap_quantiles(current_data, 10000, 0.975)
                upper[i]=np.nanmedian(btnp)
                btlower=np.zeros(200)
                btupper=np.zeros(200)
                f_l_boot = lambda d: mquantiles(d, prob=(0.025), alphap=0, betap=0)[0]
                f_u_boot = lambda d: mquantiles(d, prob=(0.975), alphap=0, betap=0)[0]
  #              f_l_boot = lambda d: harrell_davis(d, 0.025)
  #              f_u_boot = lambda d: harrell_davis(d, 0.975)
                method_lower = "Nonparametric method"
                method_upper = "Nonparametric bootstrap method"
    #            for f in tqdm(range(200)):
                for f in range(200):
                    sample_data = np.random.choice(current_data, replace=True, size=len(current_data))
                    btlower[f]=f_l_boot(sample_data)
                    btupper[f]=f_u_boot(sample_data)
                lower90_low[i] = np.percentile(btlower, 5)
                lower90_up[i] = np.percentile(btlower, 95)
                upper90_low[i] = np.percentile(btupper, 5)
                upper90_up[i] = np.percentile(btupper, 95)
            elif result[i].item()==1:
                lower[i]=robust(current_data)[0]
                upper[i]=mquantiles(current_data,prob=(0.975),alphap=0, betap=0)[0]
                btlower=np.zeros(200)
                btupper=np.zeros(200)
                f_l_boot = lambda d: robust(d)[0]
                f_u_boot = lambda d: mquantiles(d, prob=(0.975), alphap=0, betap=0)[0]
                method_lower = "Robust method"
                method_upper = "Nonparametric method"
                for f in range(200):
                    sample_data = np.random.choice(current_data, replace=True, size=len(current_data))
                    btlower[f]=f_l_boot(sample_data)
                    btupper[f]=f_u_boot(sample_data)
                lower90_low[i], lower90_up[i]=bca_correction(lower[i], btlower, current_data, f_l_boot)
                upper90_low[i], upper90_up[i]=bca_correction(upper[i], btupper, current_data, f_u_boot)
            st.image(hist[i], caption=df.columns[i], width=500)
            st.write(f' :blue[**{df.columns[i]}**]  \n  :blue[Data distribution:]  {keys[result[i].item()]}  \n  :blue[95% Reference interval:]  [{lower[i]:.3f} - {upper[i]:.3f}]  \n  :blue[90% Confidence intervals:] [{lower90_low[i]:.3f}-{lower90_up[i]:.3f} ; {upper90_low[i]:.3f}-{upper90_up[i]:.3f}]  \n  :blue[Statistical method for lower reference interval limit estimate:]  {method_lower}  \n  :blue[Statistical method for upper reference interval limit estimate:]  {method_upper}')
            if get_outlier(pd.Series(current_data))==True:
                st.write(f" :red[Some values exceed Tukey's interquartile fences: doublecheck your data for potential outliers]")


















